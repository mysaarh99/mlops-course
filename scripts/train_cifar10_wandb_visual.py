# ========================
# ğŸ“¦ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
# ========================
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
import os
import wandb
import shutil
import random  # Ù„Ø§Ø®ØªÙŠØ§Ø± Ø¹ÙŠÙ†Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©

# ========================
# ğŸš€ Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© W&B Ù„ØªØªØ¨Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨ØµØ±ÙŠÙ‹Ø§
# ========================
wandb.init(
    project="mlops_cifar10",                      # Ø§Ø³Ù… Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙÙŠ W&B
    name="cnn_cifar10_wandb_visual"               # Ø§Ø³Ù… Ø§Ù„ØªØ¬Ø±Ø¨Ø©
)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¬Ø±Ø¨Ø©
config = wandb.config
config.epochs = 5                                 # Ø¹Ø¯Ø¯ Epochs
config.batch_size = 64                            # Ø­Ø¬Ù… ÙƒÙ„ Batch
config.learning_rate = 0.001                      # Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
config.num_visual_samples = 10                    # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± Ø§Ù„ØªÙŠ Ø³Ù†Ø¹Ø±Ø¶Ù‡Ø§ Ø¨ØµØ±ÙŠÙ‹Ø§ ÙÙŠ ÙƒÙ„ Epoch

# ========================
# ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª CIFAR-10 Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ø­Ù„ÙŠÙ‹Ø§
# ========================
train_data = np.load("data/cifar10/train.npz")    # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
test_data = np.load("data/cifar10/test.npz")      # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
x_train, y_train = train_data["x"], train_data["y"]
x_test, y_test = test_data["x"], test_data["y"]

# ========================
# ğŸ·ï¸ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª ÙÙŠ CIFAR-10 (Ù„Ù„Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø¨ØµØ±ÙŠ)
# ========================
classes = [
    "airplane", "automobile", "bird", "cat", "deer",
    "dog", "frog", "horse", "ship", "truck"
]

# ========================
# ğŸ§  ØªØ¹Ø±ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ CNN Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Keras
# ========================
model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),                  # ØµÙˆØ±Ø© 32x32x3
    layers.Conv2D(32, (3, 3), activation='relu'),     # Ø·Ø¨Ù‚Ø© Convolution Ø¨Ù€ 32 ÙÙ„ØªØ±
    layers.MaxPooling2D((2, 2)),                      # Ø·Ø¨Ù‚Ø© Pooling
    layers.Conv2D(64, (3, 3), activation='relu'),     # Ø·Ø¨Ù‚Ø© Convolution Ø«Ø§Ù†ÙŠØ©
    layers.MaxPooling2D((2, 2)),                      # Ø·Ø¨Ù‚Ø© Pooling Ø«Ø§Ù†ÙŠØ©
    layers.Flatten(),                                 # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ø§ØªØ¬ Ø¥Ù„Ù‰ Ù…ØªØ¬Ù‡
    layers.Dense(64, activation='relu'),              # Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ©
    layers.Dense(10, activation='softmax')            # Ø¥Ø®Ø±Ø§Ø¬ Ø¨Ù€ 10 ÙØ¦Ø§Øª
])

# ========================
# âš™ï¸ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø®Ø³Ø§Ø±Ø© - Ø£Ù…Ø«Ù„ÙŠØ© - Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³)
# ========================
optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)
model.compile(
    optimizer=optimizer,
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# ========================
# ğŸ‹ï¸â€â™‚ï¸ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø¨ØµØ±ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… W&B
# ========================
for epoch in range(config.epochs):
    # ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Epoch ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·
    history = model.fit(
        x_train, y_train,
        epochs=1,
        batch_size=config.batch_size,
        validation_data=(x_test, y_test),
        verbose=1
    )

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¯Ø§Ø¡
    train_acc = history.history["accuracy"][-1]
    val_acc = history.history["val_accuracy"][-1]
    train_loss = history.history["loss"][-1]
    val_loss = history.history["val_loss"][-1]

    # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ ÙÙŠ W&B
    wandb.log({
        "epoch": epoch + 1,
        "train_accuracy": train_acc,
        "val_accuracy": val_acc,
        "train_loss": train_loss,
        "val_loss": val_loss
    })

    # ========================
    # ğŸ‘ï¸ ØªØªØ¨Ø¹ Ø¨ØµØ±ÙŠ - Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    # ========================
    sample_indices = random.sample(range(len(x_test)), config.num_visual_samples)
    sample_images = x_test[sample_indices]
    sample_labels = y_test[sample_indices].flatten()

    predictions = model.predict(sample_images)
    pred_labels = np.argmax(predictions, axis=1)

    wandb_images = []
    for img, true_label, pred_label in zip(sample_images, sample_labels, pred_labels):
        caption = f"True: {classes[true_label]}, Pred: {classes[pred_label]}"
        wandb_images.append(wandb.Image(img, caption=caption))

    # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ W&B Ù„ÙƒÙ„ Epoch
    wandb.log({f"visual_samples_epoch_{epoch+1}": wandb_images})

# ========================
# âœ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
# ========================
test_loss, test_acc = model.evaluate(x_test, y_test)
wandb.log({
    "test_accuracy": test_acc,
    "test_loss": test_loss
})

# ========================
# ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
# ========================
os.makedirs("models", exist_ok=True)
model_path = "models/cifar10_model_wandb_visual.keras"
model.save(model_path)

# ========================
# ğŸ—ƒï¸ Ø­ÙØ¸ Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ W&B
# ========================
wandb_artifact_path = os.path.join(wandb.run.dir, "cifar10_model_wandb_visual.keras")
shutil.copy(model_path, wandb_artifact_path)

# âœ… Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
print(f"âœ… Visual training complete (W&B). Test Accuracy: {test_acc:.4f}")
