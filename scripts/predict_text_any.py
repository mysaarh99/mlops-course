# scripts/predict_text_any.py
# -*- coding: utf-8 -*-
import os, time, json, csv, joblib, argparse
from datetime import datetime

CAND_MODELS = [
    ("models/text_baseline_model.pkl",  "models/text_baseline_vectorizer.pkl",  "baseline"),
    ("models/text_mlflow_model.pkl",    "models/text_mlflow_vectorizer.pkl",    "mlflow"),
    ("models/text_wandb_model.pkl",     "models/text_wandb_vectorizer.pkl",     "wandb"),
]

def pick_available_model():
    for m, v, name in CAND_MODELS:
        if os.path.exists(m) and os.path.exists(v):
            return m, v, name
    return None, None, None

def read_input_texts(path: str | None):
    if path and os.path.exists(path):
        texts = []
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if s:
                    texts.append(s)
        return texts
    # fallback Ø§ÙØªØ±Ø§Ø¶ÙŠ (ØºÙŠØ± Ù…ÙØ¶Ù„ Ø¹Ù„Ù…ÙŠÙ‹Ø§ØŒ Ù„ÙƒÙ† Ø¹Ù…Ù„ÙŠÙ‹Ø§ Ù„Ùˆ Ù…Ø§ ÙˆÙÙ‘Ø±Øª Ù…Ù„Ù)
    return [
        "NASA is preparing a new mission to Mars.",
        "The local hockey team won their championship game.",
        "Discussions about atheism often become controversial."
    ]

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", "-i", help="Ù…Ø³Ø§Ø± Ù…Ù„Ù Ù†ØµÙˆØµ (Ø³Ø·Ø± Ù„ÙƒÙ„ Ù†Øµ)", default=None)
    ap.add_argument("--out", "-o", help="Ù…Ø³Ø§Ø± CSV Ù„Ù„Ø¥Ø®Ø±Ø§Ø¬", default="predictions/text_any_output.csv")
    ap.add_argument("--meta", "-m", help="Ù…Ø³Ø§Ø± JSON Ù„Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§", default="predictions/text_any_meta.json")
    args = ap.parse_args()

    model_path, vect_path, tag = pick_available_model()
    if not model_path:
        raise FileNotFoundError(
            "âŒ Ù„Ù… Ø£Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ Ù†ØµÙŠ Ø£Ùˆ vectorizer.\n"
            "ØªØ£ÙƒØ¯ Ù…Ù† ØªØ´ØºÙŠÙ„ Ù…Ø±Ø§Ø­Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (baseline/mlflow/wandb) ÙˆØ£Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¯Ø§Ø®Ù„ models/."
        )

    os.makedirs(os.path.dirname(args.out), exist_ok=True)

    texts = read_input_texts(args.input)

    t0 = time.time()
    model = joblib.load(model_path)
    vectorizer = joblib.load(vect_path)
    X = vectorizer.transform(texts)
    preds = model.predict(X)

    probs = None
    if hasattr(model, "predict_proba"):
        p = model.predict_proba(X)
        probs = p.max(axis=1)

    duration = time.time() - t0

    with open(args.out, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        w.writerow(["text", "predicted_label", "confidence"])
        for i, txt in enumerate(texts):
            conf = float(probs[i]) if probs is not None else ""
            w.writerow([txt, int(preds[i]), conf])

    meta = {
        "chosen_model_tag": tag,
        "model_path": model_path,
        "vectorizer_path": vect_path,
        "n_texts": len(texts),
        "duration_sec": duration,
        "avg_sec_per_sample": (duration / len(texts)) if texts else None,
        "timestamp": datetime.utcnow().isoformat() + "Z"
    }
    with open(args.meta, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    print(f"âœ… Ø§Ø³ØªØ®Ø¯Ù…Øª Ù†Ù…ÙˆØ°Ø¬: {tag} â†’ {model_path}")
    print(f"ğŸ“„ Ø­ÙÙÙØ¸Øª Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª ÙÙŠ: {args.out}")
    print(f"ğŸ§¾ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§/Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ ÙÙŠ: {args.meta}")

if __name__ == "__main__":
    main()
